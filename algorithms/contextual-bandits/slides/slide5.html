<div class="slide-content">
    <h3>Slide 5: Interactive Demo & Code</h3>

    <h4>See LinUCB and LinTS in Action</h4>
    <p>Run the code below to compare LinUCB and LinTS on a contextual bandit problem:</p>

    <!-- PyScript Code Section -->
    <div class="code-demo">
        <h4>üìù Python Implementation</h4>
        <div class="code-container">
            <pre><code class="language-python" id="cb-code">import numpy as np
from js import document

# Contextual Bandit Environment
class ContextualBandit:
    """Linear contextual bandit with Gaussian rewards"""
    def __init__(self, n_actions=5, context_dim=10):
        self.n_actions = n_actions
        self.context_dim = context_dim
        # True parameter vectors for each action
        self.true_theta = np.random.randn(n_actions, context_dim)
        # Normalize for stability
        self.true_theta = self.true_theta / np.linalg.norm(self.true_theta, axis=1, keepdims=True)

    def get_context(self):
        """Sample a random context vector"""
        context = np.random.randn(self.context_dim)
        return context / np.linalg.norm(context)  # Normalize

    def get_reward(self, context, action):
        """Return reward for action given context"""
        expected_reward = np.dot(context, self.true_theta[action])
        noise = np.random.randn() * 0.1  # Small noise
        return expected_reward + noise

    def get_optimal_action(self, context):
        """Return the optimal action for context"""
        expected_rewards = context @ self.true_theta.T
        return np.argmax(expected_rewards)


# LinUCB Agent
class LinUCBAgent:
    """Linear Upper Confidence Bound agent"""
    def __init__(self, n_actions, context_dim, alpha=1.0):
        self.n_actions = n_actions
        self.d = context_dim
        self.alpha = alpha

        # Initialize A and b for each action
        self.A = [np.eye(context_dim) for _ in range(n_actions)]
        self.b = [np.zeros(context_dim) for _ in range(n_actions)]

    def select_action(self, context):
        """Select action using UCB"""
        ucb_values = np.zeros(self.n_actions)

        for a in range(self.n_actions):
            # Compute theta_hat
            A_inv = np.linalg.inv(self.A[a])
            theta_hat = A_inv @ self.b[a]

            # Compute UCB
            prediction = context @ theta_hat
            uncertainty = self.alpha * np.sqrt(context @ A_inv @ context)
            ucb_values[a] = prediction + uncertainty

        return np.argmax(ucb_values)

    def update(self, context, action, reward):
        """Update A and b for selected action"""
        self.A[action] += np.outer(context, context)
        self.b[action] += reward * context


# LinTS Agent
class LinTSAgent:
    """Linear Thompson Sampling agent"""
    def __init__(self, n_actions, context_dim, nu=1.0):
        self.n_actions = n_actions
        self.d = context_dim
        self.nu = nu  # Noise variance

        # Initialize A and b for each action
        self.A = [np.eye(context_dim) for _ in range(n_actions)]
        self.b = [np.zeros(context_dim) for _ in range(n_actions)]

    def select_action(self, context):
        """Select action using Thompson Sampling"""
        sampled_rewards = np.zeros(self.n_actions)

        for a in range(self.n_actions):
            # Compute posterior mean
            A_inv = np.linalg.inv(self.A[a])
            theta_hat = A_inv @ self.b[a]

            # Sample theta from posterior
            cov = self.nu * A_inv
            theta_sample = np.random.multivariate_normal(theta_hat, cov)

            # Compute expected reward with sampled theta
            sampled_rewards[a] = context @ theta_sample

        return np.argmax(sampled_rewards)

    def update(self, context, action, reward):
        """Update A and b for selected action"""
        self.A[action] += np.outer(context, context)
        self.b[action] += reward * context


# Run experiment
def run_experiment(n_rounds=500):
    """Compare LinUCB and LinTS on contextual bandit"""
    n_actions = 5
    context_dim = 10

    env = ContextualBandit(n_actions, context_dim)

    # Initialize agents
    agents = {
        'LinUCB (Œ±=1.0)': LinUCBAgent(n_actions, context_dim, alpha=1.0),
        'LinUCB (Œ±=0.5)': LinUCBAgent(n_actions, context_dim, alpha=0.5),
        'LinTS (ŒΩ=1.0)': LinTSAgent(n_actions, context_dim, nu=1.0)
    }

    # Track performance
    cumulative_rewards = {name: 0 for name in agents}
    optimal_actions = {name: 0 for name in agents}

    # Run simulation
    for t in range(n_rounds):
        context = env.get_context()
        optimal_action = env.get_optimal_action(context)

        for name, agent in agents.items():
            action = agent.select_action(context)
            reward = env.get_reward(context, action)
            agent.update(context, action, reward)

            cumulative_rewards[name] += reward
            if action == optimal_action:
                optimal_actions[name] += 1

    # Display results
    output = "CONTEXTUAL BANDIT RESULTS\n"
    output += "=" * 40 + "\n"
    output += f"Rounds: {n_rounds} | Actions: {n_actions} | Context dim: {context_dim}\n"
    output += "=" * 40 + "\n\n"

    for name in agents:
        avg_reward = cumulative_rewards[name] / n_rounds
        optimal_pct = (optimal_actions[name] / n_rounds) * 100
        output += f"{name:18s}\n"
        output += f"  Avg Reward:  {avg_reward:6.3f}\n"
        output += f"  Optimal %:   {optimal_pct:6.2f}%\n\n"

    output += "=" * 40 + "\n"
    output += "Key Insights:\n"
    output += "- LinTS often matches/beats LinUCB\n"
    output += "- No tuning needed for LinTS\n"
    output += "- Both learn the optimal policy!\n"

    document.getElementById("output").innerText = output
    print("‚úÖ Experiment completed!")


# Auto-run on load
run_experiment()
</code></pre>
        </div>

        <div class="run-section">
            <button id="run-code-cb" onclick="if(window.runPythonExperiment){window.runPythonExperiment()}else{alert('Python not ready. Please wait a moment.')}">‚ñ∂ Run Experiment</button>
            <div id="output" class="output-box">Loading Python environment...</div>
        </div>

        <!--PyScript block -->
        <script type="py">
from js import document
print("üì¶ Loading contextual bandit simulation...")

try:
    import numpy as np
    print("‚úÖ NumPy loaded!")
except ImportError as e:
    print(f"‚ùå NumPy not available: {e}")
    document.getElementById("output").innerText = "Error: NumPy failed to load."

# Contextual Bandit Environment
class ContextualBandit:
    """Linear contextual bandit with Gaussian rewards"""
    def __init__(self, n_actions=5, context_dim=10):
        self.n_actions = n_actions
        self.context_dim = context_dim
        # True parameter vectors for each action
        self.true_theta = np.random.randn(n_actions, context_dim)
        # Normalize for stability
        self.true_theta = self.true_theta / np.linalg.norm(self.true_theta, axis=1, keepdims=True)

    def get_context(self):
        """Sample a random context vector"""
        context = np.random.randn(self.context_dim)
        return context / np.linalg.norm(context)

    def get_reward(self, context, action):
        """Return reward for action given context"""
        expected_reward = np.dot(context, self.true_theta[action])
        noise = np.random.randn() * 0.1
        return expected_reward + noise

    def get_optimal_action(self, context):
        """Return the optimal action for context"""
        expected_rewards = context @ self.true_theta.T
        return np.argmax(expected_rewards)


# LinUCB Agent
class LinUCBAgent:
    """Linear Upper Confidence Bound agent"""
    def __init__(self, n_actions, context_dim, alpha=1.0):
        self.n_actions = n_actions
        self.d = context_dim
        self.alpha = alpha

        # Initialize A and b for each action
        self.A = [np.eye(context_dim) for _ in range(n_actions)]
        self.b = [np.zeros(context_dim) for _ in range(n_actions)]

    def select_action(self, context):
        """Select action using UCB"""
        ucb_values = np.zeros(self.n_actions)

        for a in range(self.n_actions):
            # Compute theta_hat
            A_inv = np.linalg.inv(self.A[a])
            theta_hat = A_inv @ self.b[a]

            # Compute UCB
            prediction = context @ theta_hat
            uncertainty = self.alpha * np.sqrt(context @ A_inv @ context)
            ucb_values[a] = prediction + uncertainty

        return np.argmax(ucb_values)

    def update(self, context, action, reward):
        """Update A and b for selected action"""
        self.A[action] += np.outer(context, context)
        self.b[action] += reward * context


# LinTS Agent
class LinTSAgent:
    """Linear Thompson Sampling agent"""
    def __init__(self, n_actions, context_dim, nu=1.0):
        self.n_actions = n_actions
        self.d = context_dim
        self.nu = nu

        # Initialize A and b for each action
        self.A = [np.eye(context_dim) for _ in range(n_actions)]
        self.b = [np.zeros(context_dim) for _ in range(n_actions)]

    def select_action(self, context):
        """Select action using Thompson Sampling"""
        sampled_rewards = np.zeros(self.n_actions)

        for a in range(self.n_actions):
            # Compute posterior mean
            A_inv = np.linalg.inv(self.A[a])
            theta_hat = A_inv @ self.b[a]

            # Sample theta from posterior
            cov = self.nu * A_inv
            theta_sample = np.random.multivariate_normal(theta_hat, cov)

            # Compute expected reward with sampled theta
            sampled_rewards[a] = context @ theta_sample

        return np.argmax(sampled_rewards)

    def update(self, context, action, reward):
        """Update A and b for selected action"""
        self.A[action] += np.outer(context, context)
        self.b[action] += reward * context


# Run experiment
def run_experiment(n_rounds=500):
    """Compare LinUCB and LinTS on contextual bandit"""
    print(f"üîÑ Starting new experiment with {n_rounds} rounds...")
    document.getElementById("output").innerText = "Running experiment..."

    n_actions = 5
    context_dim = 10

    env = ContextualBandit(n_actions, context_dim)

    # Initialize agents
    agents = {
        'LinUCB (Œ±=1.0)': LinUCBAgent(n_actions, context_dim, alpha=1.0),
        'LinUCB (Œ±=0.5)': LinUCBAgent(n_actions, context_dim, alpha=0.5),
        'LinTS (ŒΩ=1.0)': LinTSAgent(n_actions, context_dim, nu=1.0)
    }

    # Track performance
    cumulative_rewards = {name: 0 for name in agents}
    optimal_actions = {name: 0 for name in agents}

    # Run simulation
    for t in range(n_rounds):
        context = env.get_context()
        optimal_action = env.get_optimal_action(context)

        for name, agent in agents.items():
            action = agent.select_action(context)
            reward = env.get_reward(context, action)
            agent.update(context, action, reward)

            cumulative_rewards[name] += reward
            if action == optimal_action:
                optimal_actions[name] += 1

    # Display results
    output = "CONTEXTUAL BANDIT RESULTS\n"
    output += "=" * 40 + "\n"
    output += f"Rounds: {n_rounds} | Actions: {n_actions} | Context dim: {context_dim}\n"
    output += "=" * 40 + "\n\n"

    for name in agents:
        avg_reward = cumulative_rewards[name] / n_rounds
        optimal_pct = (optimal_actions[name] / n_rounds) * 100
        output += f"{name:18s}\n"
        output += f"  Avg Reward:  {avg_reward:6.3f}\n"
        output += f"  Optimal %:   {optimal_pct:6.2f}%\n\n"

    output += "=" * 40 + "\n"
    output += "Key Insights:\n"
    output += "- LinTS often matches/beats LinUCB\n"
    output += "- No tuning needed for LinTS\n"
    output += "- Both learn the optimal policy!\n"

    document.getElementById("output").innerText = output
    print("‚úÖ Experiment completed!")


# Run on load
print("üéØ Running initial experiment...")
try:
    run_experiment()
    print("‚úÖ Ready! Click 'Run Experiment' to run again.")
except Exception as e:
    document.getElementById("output").innerText = f"Error: {str(e)}"
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

# Expose to JavaScript
from pyodide.ffi import create_proxy
from js import window

js_run_experiment = create_proxy(run_experiment)
window.runPythonExperiment = js_run_experiment

print("‚ú® Function exposed as window.runPythonExperiment")
print("üì¢ Ready to run experiments! Click the button to run again.")
        </script>
    </div>

    <div class="math-box">
        <p><strong>What's happening in the simulation:</strong></p>
        <ul>
            <li>Environment generates random contexts <span class="math">\mathbf{x}_t \in \mathbb{R}^{10}</span></li>
            <li>Each action has a true parameter vector <span class="math">\boldsymbol{\theta}_a^*</span></li>
            <li>Reward = <span class="math">\mathbf{x}_t^\top \boldsymbol{\theta}_a^*</span> + noise</li>
            <li>Agents learn <span class="math">\boldsymbol{\theta}_a</span> online and select actions</li>
            <li>We track cumulative reward and optimal action selection rate</li>
        </ul>
    </div>

    <div class="learning-note">
        <h4>üî¨ Experiment & Learn</h4>
        <p>
            <strong>Try modifying the code!</strong> Some experiments to try:
        </p>
        <ul>
            <li>Change <code>n_rounds</code> to 1000 or 2000 - do the algorithms converge?</li>
            <li>Increase <code>context_dim</code> to 20 - how does performance change?</li>
            <li>Add more actions (<code>n_actions = 10</code>) - is it harder to learn?</li>
            <li>Try different Œ± values for LinUCB (0.1, 2.0) - which is best?</li>
            <li>Modify the noise in <code>get_reward()</code> - does high noise hurt performance?</li>
        </ul>

        <p>
            <strong>Expected observations:</strong>
        </p>
        <ul>
            <li>LinTS typically achieves higher optimal action % without parameter tuning</li>
            <li>LinUCB with Œ±=1.0 is a good default but may need adjustment</li>
            <li>Both algorithms should achieve 80%+ optimal actions after sufficient rounds</li>
            <li>Higher context dimension requires more samples to learn accurately</li>
        </ul>

        <p>
            <strong>Real-world insight:</strong> This simulation mimics personalized recommendation
            systems where the context is user features and actions are items to recommend. The
            algorithms learn which items work best for which types of users!
        </p>
    </div>
</div>
