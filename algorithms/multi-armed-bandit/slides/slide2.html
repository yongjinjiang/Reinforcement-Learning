<div class="slide-content">
    <h3>Slide 2: Îµ-Greedy Strategy</h3>

    <h4>Simple but Effective Approach</h4>
    <ul>
        <li>With probability <strong>Îµ</strong>: explore (choose random action)</li>
        <li>With probability <strong>1-Îµ</strong>: exploit (choose best known action)</li>
        <li>Maintain running averages of action values: <span class="math">Q_t(a)</span></li>
    </ul>

    <div class="algorithm-box">
        <p><strong>Action Selection Policy:</strong></p>
        <div class="math-display">
            A_t = \begin{cases}
            \arg\max_a Q_t(a) & \text{with probability } 1-\varepsilon \\
            \text{random action} & \text{with probability } \varepsilon
            \end{cases}
        </div>
    </div>

    <div class="math-box">
        <p><strong>Incremental Update Rule:</strong></p>
        <div class="math-display">
            Q_{t+1}(a) = Q_t(a) + \alpha \left[ R_t - Q_t(a) \right]
        </div>
        <p class="math-note">
            where <span class="math">\alpha \in (0, 1]</span> is the step size (learning rate)
        </p>

        <p class="math-note">
            <strong>Why this works:</strong> Can be rewritten as:
        </p>
        <div class="math-display">
            Q_{t+1}(a) = (1-\alpha) Q_t(a) + \alpha R_t
        </div>
        <p class="math-note">
            This is an exponentially weighted average - recent rewards have weight
            <span class="math">\alpha</span>, older rewards decay by factor
            <span class="math">(1-\alpha)</span>
        </p>
    </div>

    <div class="algorithm-box">
        <p><strong>Sample Average (Î± = 1/n):</strong></p>
        <div class="math-display">
            Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} \left[ R_t - Q_t(a) \right]
        </div>
        <p class="math-note">
            where <span class="math">N_t(a)</span> is the number of times action
            <span class="math">a</span> has been selected
        </p>
    </div>

    <div class="learning-note">
        <h4>ðŸ¤” My Learning Notes</h4>
        <p>
            <strong>Insight:</strong> The update rule is a form of <em>stochastic gradient descent</em>!
            We're moving our estimate in the direction of the observed reward.
        </p>
        <p>
            <strong>Connection to physics:</strong> Similar to a damped oscillator - recent
            observations have more "momentum" in shaping our estimates. The step size
            <span class="math">\alpha</span> acts like a damping coefficient.
        </p>

        <p>
            <strong>Q: Why does the "Incremental Update Rule" have a tunable Î± parameter while the "Action-Value Estimate" has none?</strong>
        </p>
        <p>
            <strong>A:</strong> These represent two different approaches to averaging rewards:
        </p>
        <ul>
            <li>
                <strong>Sample Average Method</strong> (from Slide 1): No tunable parameters.
                Computes the true average: <span class="math">Q_t(a) = \frac{\text{sum of rewards}}{\text{count}}</span>.
                All past rewards are weighted equally.
            </li>
            <li>
                <strong>Incremental Update with constant Î±</strong>: Creates an <em>exponentially weighted average</em>
                where recent rewards have more influence. Older rewards exponentially decay by factor <span class="math">(1-\alpha)</span>.
            </li>
        </ul>

        <p>
            <strong>The Connection:</strong> The incremental rule can represent BOTH approaches:
        </p>
        <ul>
            <li>
                When <span class="math">\alpha = \frac{1}{N_t(a)}</span> (decreasing step size),
                the incremental rule is equivalent to the sample average method
            </li>
            <li>
                When <span class="math">\alpha</span> is constant (e.g., 0.1),
                you get an exponentially weighted average that "forgets" old observations
            </li>
        </ul>

        <p>
            <strong>When to use constant Î± vs sample average:</strong>
        </p>
        <ul>
            <li>
                <strong>Constant Î±:</strong> Better for <em>non-stationary</em> environments
                where reward distributions change over time. Allows adaptation by giving more weight to recent data.
            </li>
            <li>
                <strong>Sample average (Î± = 1/N):</strong> Better for <em>stationary</em> environments
                where true means are fixed. Converges to the true expected value.
            </li>
        </ul>

        <p>
            <strong>Î± trade-off:</strong> Small <span class="math">\alpha</span> (e.g., 0.01) = slow learning,
            stable estimates. Large <span class="math">\alpha</span> (e.g., 0.5) = fast adaptation,
            noisy estimates.
        </p>

        <p>
            <strong>Îµ trade-off:</strong> Large <span class="math">\varepsilon</span> (e.g., 0.1)
            explores more but exploits less. Small <span class="math">\varepsilon</span> (e.g., 0.01)
            exploits more but risks missing better actions.
        </p>
        <p>
            <strong>Common practice:</strong> Use decaying <span class="math">\varepsilon_t = \frac{1}{t}</span>
            to explore more early, exploit more later.
        </p>
    </div>
</div>
