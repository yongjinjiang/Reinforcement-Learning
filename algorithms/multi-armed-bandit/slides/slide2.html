<div class="slide-content">
    <h3>Slide 2: Îµ-Greedy Strategy</h3>

    <h4>Simple but Effective Approach</h4>
    <ul>
        <li>With probability <strong>Îµ</strong>: explore (choose random action)</li>
        <li>With probability <strong>1-Îµ</strong>: exploit (choose best known action)</li>
        <li>Maintain running averages of action values: <span class="math">Q_t(a)</span></li>
    </ul>

    <div class="algorithm-box">
        <p><strong>Action Selection Policy:</strong></p>
        <div class="math-display">
            A_t = \begin{cases}
            \arg\max_a Q_t(a) & \text{with probability } 1-\varepsilon \\
            \text{random action} & \text{with probability } \varepsilon
            \end{cases}
        </div>
    </div>

    <div class="math-box">
        <p><strong>Incremental Update Rule:</strong></p>
        <div class="math-display">
            Q_{t+1}(a) = Q_t(a) + \alpha \left[ R_t - Q_t(a) \right]
        </div>
        <p class="math-note">
            where <span class="math">\alpha \in (0, 1]</span> is the step size (learning rate)
        </p>

        <p class="math-note">
            <strong>Why this works:</strong> Can be rewritten as:
        </p>
        <div class="math-display">
            Q_{t+1}(a) = (1-\alpha) Q_t(a) + \alpha R_t
        </div>
        <p class="math-note">
            This is an exponentially weighted average - recent rewards have weight
            <span class="math">\alpha</span>, older rewards decay by factor
            <span class="math">(1-\alpha)</span>
        </p>
    </div>

    <div class="algorithm-box">
        <p><strong>Sample Average (Î± = 1/n):</strong></p>
        <div class="math-display">
            Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} \left[ R_t - Q_t(a) \right]
        </div>
        <p class="math-note">
            where <span class="math">N_t(a)</span> is the number of times action
            <span class="math">a</span> has been selected
        </p>
    </div>

    <div class="learning-note">
        <h4>ðŸ¤” My Learning Notes</h4>
        <p>
            <strong>Insight:</strong> The update rule is a form of <em>stochastic gradient descent</em>!
            We're moving our estimate in the direction of the observed reward.
        </p>
        <p>
            <strong>Connection to physics:</strong> Similar to a damped oscillator - recent
            observations have more "momentum" in shaping our estimates. The step size
            <span class="math">\alpha</span> acts like a damping coefficient.
        </p>
        <p>
            <strong>Îµ trade-off:</strong> Large <span class="math">\varepsilon</span> (e.g., 0.1)
            explores more but exploits less. Small <span class="math">\varepsilon</span> (e.g., 0.01)
            exploits more but risks missing better actions.
        </p>
        <p>
            <strong>Common practice:</strong> Use decaying <span class="math">\varepsilon_t = \frac{1}{t}</span>
            to explore more early, exploit more later.
        </p>
    </div>
</div>
