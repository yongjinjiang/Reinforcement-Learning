<div class="slide-content">
    <h3>Slide 4: Thompson Sampling</h3>

    <h4>Bayesian Approach to Exploration</h4>
    <p>
        Thompson Sampling is a <em>probability matching</em> strategy that uses Bayesian inference
        to balance exploration and exploitation. It's elegant, practical, and often outperforms UCB.
    </p>

    <div class="algorithm-box">
        <p><strong>Thompson Sampling Algorithm:</strong></p>
        <ol>
            <li>Maintain a <em>posterior distribution</em> over each arm's reward probability</li>
            <li>At each time step:
                <ul>
                    <li>Sample a value from each arm's posterior distribution</li>
                    <li>Select the arm with the highest sampled value</li>
                    <li>Observe reward and update the posterior using Bayes' rule</li>
                </ul>
            </li>
        </ol>
    </div>

    <div class="math-box">
        <p><strong>For Bernoulli Bandits (Beta-Bernoulli Model):</strong></p>

        <p class="math-note">
            <strong>Prior:</strong> For each arm <span class="math">a</span>, start with
            Beta distribution <span class="math">\text{Beta}(\alpha_a, \beta_a)</span>
        </p>

        <div class="math-display">
            \theta_a \sim \text{Beta}(\alpha_a, \beta_a)
        </div>

        <p class="math-note">
            <strong>Action Selection:</strong> Sample from each posterior and pick the best
        </p>
        <div class="math-display">
            \tilde{\theta}_a \sim \text{Beta}(\alpha_a, \beta_a) \quad \forall a
        </div>
        <div class="math-display">
            A_t = \arg\max_a \tilde{\theta}_a
        </div>

        <p class="math-note">
            <strong>Posterior Update:</strong> After observing reward <span class="math">r \in \{0, 1\}</span>
        </p>
        <div class="math-display">
            \alpha_a \leftarrow \alpha_a + r
        </div>
        <div class="math-display">
            \beta_a \leftarrow \beta_a + (1 - r)
        </div>
    </div>

    <div class="math-box">
        <p><strong>Why Beta Distribution?</strong></p>
        <ul>
            <li>
                <strong>Conjugate prior:</strong> Beta is conjugate to Bernoulli, making updates simple and exact
            </li>
            <li>
                <strong>Interpretable parameters:</strong>
                <ul>
                    <li><span class="math">\alpha_a</span> = number of successes (reward = 1) + prior successes</li>
                    <li><span class="math">\beta_a</span> = number of failures (reward = 0) + prior failures</li>
                </ul>
            </li>
            <li>
                <strong>Mean estimate:</strong>
                <span class="math">\mathbb{E}[\theta_a] = \frac{\alpha_a}{\alpha_a + \beta_a}</span>
                (similar to sample average!)
            </li>
            <li>
                <strong>Uncertainty:</strong> When <span class="math">\alpha_a + \beta_a</span> is small,
                variance is high â†’ encourages exploration
            </li>
        </ul>
    </div>

    <div class="math-box">
        <p><strong>Probability Matching Property:</strong></p>
        <div class="math-display">
            P(A_t = a) \approx P(a \text{ is optimal} | \text{history})
        </div>
        <p class="math-note">
            Thompson Sampling naturally selects each arm with probability proportional to
            the probability that it's the best arm. This is optimal in a Bayesian sense!
        </p>
    </div>

    <div class="learning-note">
        <h4>ðŸ¤” My Learning Notes</h4>

        <p>
            <strong>Why is Thompson Sampling so elegant?</strong> It doesn't need tunable parameters
            like <span class="math">\varepsilon</span> or <span class="math">c</span>. The exploration
            happens naturally through the uncertainty in the posterior distributions.
        </p>

        <p>
            <strong>Intuition:</strong> When we're uncertain about an arm (few observations),
            the Beta distribution has high variance. Sampling from it can produce high values,
            making us try that arm. As we gather data, the distribution concentrates around
            the true value, reducing exploration of that arm.
        </p>

        <p>
            <strong>Q: How does Thompson Sampling compare to UCB?</strong>
        </p>
        <ul>
            <li>
                <strong>Thompson Sampling:</strong> Probabilistic, Bayesian, parameter-free,
                often better empirical performance
            </li>
            <li>
                <strong>UCB:</strong> Deterministic, frequentist, requires tuning <span class="math">c</span>,
                stronger theoretical guarantees
            </li>
            <li>
                Both achieve <span class="math">O(\log T)</span> regret!
            </li>
        </ul>

        <p>
            <strong>The Beta-Bernoulli trick:</strong> Starting with uniform prior
            <span class="math">\text{Beta}(1, 1)</span> is like assuming we've seen 1 success
            and 1 failure before. This prevents over-commitment to early observations.
        </p>

        <p>
            <strong>Connection to statistics:</strong> This is essentially
            <em>Bayesian A/B testing</em>! Many modern A/B testing platforms use Thompson Sampling
            because it naturally handles the exploration-exploitation tradeoff.
        </p>

        <p>
            <strong>Extensions:</strong> Thompson Sampling generalizes to other distributions:
        </p>
        <ul>
            <li>Gaussian rewards â†’ Normal-Gamma posterior</li>
            <li>Count data â†’ Gamma-Poisson posterior</li>
            <li>Contextual bandits â†’ Bayesian regression</li>
        </ul>

        <p>
            <strong>Historical note:</strong> Proposed in 1933 by William Thompson, but only recently
            (2010s) gained popularity due to its strong empirical performance and theoretical
            properties being proven.
        </p>

        <p>
            <strong>Real-world success:</strong> Used extensively in industry for:
        </p>
        <ul>
            <li>Online advertising (ad selection)</li>
            <li>Recommender systems (content ranking)</li>
            <li>Clinical trials (adaptive treatment assignment)</li>
            <li>Website optimization (multivariate testing)</li>
        </ul>
    </div>
</div>
